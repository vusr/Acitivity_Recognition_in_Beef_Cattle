{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22364cf-cf4e-40e4-b36f-7ec6d602a51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define the folder to save plots\n",
    "save_folder = 'saved_plots_6_classes'\n",
    "os.makedirs(save_folder, exist_ok=True)  # Create the folder if it doesn't exist\n",
    "\n",
    "# Choose computation device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load pre-trained CLIP model\n",
    "model, preprocess = clip.load('ViT-L/14@336px', device=device, jit=False)\n",
    "\n",
    "# Define custom dataset\n",
    "class ImageTitleDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, class_to_idx):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.tokenized_labels = clip.tokenize(labels)  # Tokenize labels for CLIP\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = preprocess(Image.open(self.image_paths[idx]))\n",
    "        label = self.tokenized_labels[idx]\n",
    "        return image, label, self.labels[idx]  # Return image, tokenized text, and label\n",
    "\n",
    "# Prepare dataset and class-wise splitting\n",
    "folder_path = \"labeled_activity_dataset_6_classes/\"\n",
    "folders = os.listdir(folder_path)\n",
    "\n",
    "class_to_idx = {cls_name: idx for idx, cls_name in enumerate(folders)}\n",
    "train_images, val_images, test_images = [], [], []\n",
    "train_labels, val_labels, test_labels = [], [], []\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path_full = os.path.join(folder_path, folder)\n",
    "    images_in_folder = os.listdir(folder_path_full)\n",
    "    image_paths = [os.path.join(folder_path_full, img) for img in images_in_folder]\n",
    "\n",
    "    # Split 2:1:1 for train, val, test\n",
    "    train, temp = train_test_split(image_paths, test_size=0.5, random_state=42)\n",
    "    val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_images.extend(train)\n",
    "    val_images.extend(val)\n",
    "    test_images.extend(test)\n",
    "\n",
    "    train_labels.extend([folder] * len(train))\n",
    "    val_labels.extend([folder] * len(val))\n",
    "    test_labels.extend([folder] * len(test))\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ImageTitleDataset(train_images, train_labels, class_to_idx)\n",
    "val_dataset = ImageTitleDataset(val_images, val_labels, class_to_idx)\n",
    "test_dataset = ImageTitleDataset(test_images, test_labels, class_to_idx)\n",
    "\n",
    "# Custom batch sampler to ensure one image per class in each batch\n",
    "class ClassBalancedBatchSampler:\n",
    "    def __init__(self, dataset, class_to_idx):\n",
    "        self.dataset = dataset\n",
    "        self.class_to_idx = class_to_idx\n",
    "        self.class_to_image_idx = {cls: [] for cls in class_to_idx.keys()}\n",
    "\n",
    "        # Group indices by class\n",
    "        for idx, (_, _, label) in enumerate(dataset):\n",
    "            self.class_to_image_idx[label].append(idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Create batches ensuring one sample per class in each batch\n",
    "        min_class_size = min(len(indices) for indices in self.class_to_image_idx.values())\n",
    "        for i in range(min_class_size):\n",
    "            batch_indices = [\n",
    "                self.class_to_image_idx[cls][i % len(self.class_to_image_idx[cls])]\n",
    "                for cls in self.class_to_idx.keys()\n",
    "            ]\n",
    "            yield batch_indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(indices) for indices in self.class_to_image_idx.values())\n",
    "\n",
    "train_sampler = ClassBalancedBatchSampler(train_dataset, class_to_idx)\n",
    "val_sampler = ClassBalancedBatchSampler(val_dataset, class_to_idx)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler)\n",
    "val_loader = DataLoader(val_dataset, batch_sampler=val_sampler)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-6, betas=(0.9, 0.98), eps=1e-6, weight_decay=0.2)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5  # Number of epochs to wait for improvement\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training and validation with early stopping\n",
    "num_epochs = 100\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for images, texts, labels in train_loader:\n",
    "        images = images.to(device)  # Move images to device\n",
    "        texts = texts.to(device)    # Move tokenized texts to device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_per_image, logits_per_text = model(images, texts)\n",
    "        ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "        loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, texts, labels in val_loader:\n",
    "            images = images.to(device)  # Move images to the computation device\n",
    "            texts = texts.to(device)    # Move tokenized texts to the computation device\n",
    "\n",
    "            logits_per_image, logits_per_text = model(images, texts)\n",
    "            ground_truth = torch.arange(len(images), dtype=torch.long, device=device)\n",
    "            loss = (loss_fn(logits_per_image, ground_truth) + loss_fn(logits_per_text, ground_truth)) / 2\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_losses[-1]:.4f}, Val Loss: {val_losses[-1]:.4f}\")\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_losses[-1] < best_val_loss:\n",
    "        best_val_loss = val_losses[-1]\n",
    "        patience_counter = 0  # Reset counter if validation loss improves\n",
    "        # Optionally, save the best model\n",
    "        torch.save(model.state_dict(), \"best_model_6_classes.pth\")\n",
    "    else:\n",
    "        patience_counter += 1  # Increment counter if no improvement\n",
    "        print(f\"Early stopping patience counter: {patience_counter}/{patience}\")\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered. Stopping training.\")\n",
    "            break\n",
    "\n",
    "# Load the best model for evaluation\n",
    "model.load_state_dict(torch.load(\"best_model_6_classes.pth\"))\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(len(train_losses)), train_losses, label=\"Train Loss\")\n",
    "plt.plot(range(len(val_losses)), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.savefig(os.path.join(save_folder, \"train_val_loss.png\"))  # Save plot\n",
    "\n",
    "# Test Phase\n",
    "model.eval()\n",
    "all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "# Tokenize all class descriptions once for efficiency\n",
    "class_texts = clip.tokenize(list(class_to_idx.keys())).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for image_path, label in zip(test_images, test_labels):\n",
    "        # Load and preprocess the test image\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "        # Encode the image and text using the model\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(class_texts)\n",
    "\n",
    "        # Compute logits and probabilities\n",
    "        logits_per_image, logits_per_text = model(image, class_texts)\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "        # Get the top predicted class\n",
    "        pred_idx = logits_per_image.argmax(dim=1).item()\n",
    "        pred_label = list(class_to_idx.keys())[pred_idx]\n",
    "\n",
    "        # Store predictions and true labels\n",
    "        all_preds.append(pred_label)\n",
    "        all_labels.append(label)\n",
    "        all_probs.append(probs[0])\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Map class names to numeric labels\n",
    "numeric_class_labels = list(range(len(class_to_idx)))\n",
    "label_to_numeric = {label: idx for idx, label in enumerate(class_to_idx.keys())}\n",
    "numeric_all_labels = [label_to_numeric[label] for label in all_labels]\n",
    "numeric_all_preds = [label_to_numeric[label] for label in all_preds]\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(numeric_all_labels, numeric_all_preds, labels=numeric_class_labels)\n",
    "disp = ConfusionMatrixDisplay(cm, display_labels=numeric_class_labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.savefig(os.path.join(save_folder, \"confusion_matrix.png\"))  # Save confusion matrix plot\n",
    "\n",
    "# Display class names beside the matrix\n",
    "plt.gca().figure.subplots_adjust(left=0.25)  # Adjust for extra space\n",
    "for i, class_name in enumerate(class_to_idx.keys()):\n",
    "    plt.text(-2.5, i, f\"{class_name}\", rotation=0, va=\"center\", ha=\"right\", fontsize=10)\n",
    "plt.savefig(os.path.join(save_folder, \"confusion_matrix_with_labels.png\"))  # Save updated confusion matrix plot\n",
    "\n",
    "# Group test images by true class\n",
    "test_images_grouped = {class_name: [] for class_name in class_to_idx.keys()}\n",
    "for img_path, label in zip(test_images, all_labels):\n",
    "    test_images_grouped[label].append(img_path)\n",
    "\n",
    "# Plot grouped images with predicted class names\n",
    "for class_name, images in test_images_grouped.items():\n",
    "    plt.figure(figsize=(16, len(images) // 4 * 4))  # Adjust figure size based on number of images\n",
    "    plt.suptitle(f\"Class: {class_name} (Numeric Label: {class_to_idx[class_name]})\", fontsize=16, y=1.02)\n",
    "\n",
    "    for idx, img_path in enumerate(images):\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        # Get predicted label for this image\n",
    "        try:\n",
    "            img_index = test_images.index(img_path)  # Ensure img_path exists in test_images\n",
    "            pred_label = all_preds[img_index]  # This is already a string (the predicted class)\n",
    "        except ValueError:\n",
    "            pred_label = \"Unknown\"  # If img_path is not found in test_images\n",
    "        \n",
    "        # Determine if the prediction is correct\n",
    "        is_correct = pred_label == class_name  # Compare predicted label with true label\n",
    "\n",
    "        # Set color for title based on correctness\n",
    "        title_color = \"green\" if is_correct else \"red\"\n",
    "\n",
    "        plt.subplot((len(images) + 3) // 4, 4, idx + 1)  # 4 columns\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Pred: {pred_label}\", fontsize=10, color=title_color, pad=5)\n",
    "        plt.axis(\"off\")\n",
    "    \n",
    "    # Save the figure\n",
    "    save_path = os.path.join(save_folder, f\"{class_name}_grouped_images.png\")\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Make space for the title\n",
    "    plt.savefig(save_path)  # Save plot to the folder\n",
    "    plt.show()  # Close the plot to avoid display in the notebook or further processing\n",
    "\n",
    "print(\"All plots saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9a91a1-3068-4bc5-bd6c-e7d5210d82c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
